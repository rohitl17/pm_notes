# âš—ï¸ Designing and Running Experiments

---

## ğŸ§© What is an MVP?

- **Minimum Viable Product (MVP)** is the **smallest version** of a product that allows you to **test a core assumption** with real users.  
- Its purpose is to **learn**, not to launch.  
- MVPs reduce **risk, cost, and time** by validating whether users find value in your idea **before full-scale development**.  
- MVPs help answer:  
  > â€œDoes this problem really exist?â€ and â€œDoes our solution actually solve it?â€

**Key Principle:**  
> Build to **learn**, not to **scale**.

---

## ğŸ’­ How Do Product Managers Think About MVPs?

- PMs use MVPs to **validate assumptions quickly** and **de-risk decisions**.  
- MVPs are **experiments**, not prototypes â€” they must test a **real hypothesis**.  
- Good MVPs:
  - Focus on **one clear learning goal**  
  - Deliver a **real user experience** (not just theoretical feedback)  
  - Measure a **defined success metric**  

- PM mindset:
  - **â€œWhatâ€™s the fastest way to learn?â€** instead of â€œWhatâ€™s the fastest way to build?â€
  - Use MVPs to test **behavior**, not opinions.

> Example: Instead of asking, â€œWould you use this feature?â€, observe whether users actually click, sign up, or pay.

---

## ğŸ§­ Seven Steps to Running an MVP Experiment

1. **Define Objective**  
   - What do you want to learn? (e.g., user need, feature adoption, pricing fit)

2. **Identify Assumptions**  
   - What must be true for your idea to succeed?

3. **Formulate Hypotheses**  
   - Convert assumptions into testable statements.  
   - Example: â€œIf we offer free setup, 40% of users will complete onboarding.â€

4. **Design the MVP**  
   - Choose the smallest experiment to test the hypothesis.  
   - Examples: landing page, email test, concierge MVP.

5. **Set Success Criteria (MCS)**  
   - Define the **Minimum Criterion for Success** before launching.  
   - Prevents bias in interpreting results.

6. **Run the Experiment**  
   - Launch quickly, collect behavioral data, and observe patterns.

7. **Analyze and Learn**  
   - Evaluate outcomes: Confirmed âœ… | Contradicted âŒ | Inconclusive âšª  
   - Use insights to iterate or pivot.

---

## ğŸ§  Identify Your Assumptions

- Assumptions are **beliefs that must hold true** for your product to succeed.  
- Identifying them early helps you **prioritize risks** and design better experiments.

**Types of Assumptions:**
1. **Desirability:** Do users actually want this?  
2. **Feasibility:** Can we build it effectively?  
3. **Viability:** Is it sustainable or profitable?  
4. **Usability:** Can users understand and use it easily?

### Example Framework

| **Assumption Type** | **Example** | **Test Method** |
|----------------------|-------------|-----------------|
| Desirability | â€œUsers want same-day deliveryâ€ |  landing page |
| Feasibility | â€œWe can process requests in <2 hrsâ€ | Internal prototype |
| Viability | â€œPeople will pay $10/monthâ€ | Pricing test |
| Usability | â€œSignup takes <2 minâ€ | Usability test |

> Tip: Focus first on **the riskiest assumption** â€” the one most likely to break your business if false.

---

## ğŸ§© Case Study: Identify the Assumptions for Zirx

**Zirx** (example company) â€” an on-demand car service that picks up, parks, and returns your car.  
Identified assumptions include:

- **Customers feel real pain** finding parking in urban centers.  
- **Users trust** third-party drivers to handle their vehicles.  
- **Customers are willing to pay** for convenience.  
- **Drivers and logistics** can operate efficiently in dense cities.  
- **Demand density** is sufficient to make routes economical.  

> Key learning: List every assumption, no matter how obvious. You canâ€™t test what you havenâ€™t written down.

---

## âš ï¸ Find the Riskiest Assumption of Them All

- The **riskiest assumption** is the one that, if false, makes the entire product fail.  
- Often related to **user demand**, **trust**, or **behavioral change**.

### How to Identify It
1. List all assumptions.
2. Rank them by **impact** (on product success) and **uncertainty** (how little you know).  
3. The assumption with **highest impact + highest uncertainty** = riskiest.

> Example (Zirx): â€œWill customers actually trust someone else to drive and park their car?â€

---

## ğŸ”² Make Decisions: The Risk/Difficulty Square

Use this matrix to **prioritize what to test first**.

| | **Low Difficulty** | **High Difficulty** |
|--|--------------------|---------------------|
| **High Risk** | âœ… Test immediately | âš ï¸ Requires creative experiment design |
| **Low Risk** | ğŸ•’ Test later | ğŸš« Probably not worth testing |

> Start with **High-Risk + Low-Difficulty** items.  
> They provide the fastest, cheapest learning loops.

---

## ğŸ§ª What is a Hypothesis?

A **hypothesis** is a **testable statement** that connects an assumption to an expected outcome.  
It translates uncertainty into something measurable.

**Formula:**  
> _â€œIf we [do X], then [Y] will happen because [reason].â€_

Example:  
> â€œIf we add a â€˜Book Nowâ€™ button on the homepage, then at least 20% of visitors will click it, because theyâ€™re interested in on-demand parking.â€

---

## ğŸ§° Put Together a Hypothesis

### Steps
1. **Start from your assumption.**  
   â†’ â€œCustomers want faster parking options.â€
2. **Define an action or intervention.**  
   â†’ â€œShow an ad describing our pickup & park service.â€
3. **Define measurable outcome.**  
   â†’ â€œAt least 25% click â€˜Learn Moreâ€™.â€

**Good Hypotheses Are:**
- Specific and measurable  
- Based on a single assumption  
- Disprovable (can fail)  
- Time-bound and clear

---

## ğŸš— Follow Along: Identify Zirxâ€™s Hypothesis

Example Hypothesis:
> â€œIf we run Facebook ads targeting city commuters, 10% will sign up for early access because they want to avoid parking stress.â€

- **Metric:** Click-through and sign-up rate  
- **Goal:** Validate **desirability** â€” whether the problem is worth solving  
- **Next Step:** Build a minimal landing page with signup form

---

## ğŸ¯ Whatâ€™s a Minimum Criterion for Success (MCS)?

- The **MCS** defines what success looks like for your MVP experiment.  
- It prevents **bias** in interpreting results after the fact.  
- MCS should be **quantitative**, **predefined**, and **tied to your hypothesis**.

**Example:**  
> â€œAt least 20% of people who click the ad should sign up for early access.â€

Without an MCS, any result can be rationalized as â€œinteresting,â€ not â€œvalidated.â€

---

## ğŸ“ Create a Formula for Your MCS

To formalize success criteria:

> **MCS = Minimum % or Count that Confirms Hypothesis**

Example:
> â€œWe will consider this test a success if at least 15% of visitors sign up, with a sample size of 200 visitors.â€

### Tips
- Ensure **statistical significance** for meaningful conclusions  
- Document assumptions: sample size, timeframe, conversion goals  
- Always define MCS **before running** the experiment

---

## ğŸ’¡ Make the Calculation for Startups

For early-stage startups, use **smaller, directional MCS** metrics:
- Focus on **signal, not scale**
- Measure **behavioral intent**, not growth metrics
- Example:
  - 20 out of 100 visitors sign up â†’ positive signal  
  - 1 out of 100 â†’ rethink messaging or value prop

> Early MCS â‰  proof of business model; itâ€™s **evidence of user interest**.

---

## ğŸ“Š Evaluating Results and Learning from Them

1. **Compare results** against your hypothesis and MCS  
2. **Categorize outcomes:**
   - âœ… Confirmed  
   - âŒ Disproved  
   - âšª Inconclusive  
3. **Document findings:**
   - Key insights  
   - New assumptions  
   - Next experiments

> The goal is not to be right â€” itâ€™s to **learn faster** and **reduce uncertainty**.

