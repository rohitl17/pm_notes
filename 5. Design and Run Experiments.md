# ⚗️ Designing and Running Experiments

---

## 🧩 What is an MVP?

- **Minimum Viable Product (MVP)** is the **smallest version** of a product that allows you to **test a core assumption** with real users.  
- Its purpose is to **learn**, not to launch.  
- MVPs reduce **risk, cost, and time** by validating whether users find value in your idea **before full-scale development**.  
- MVPs help answer:  
  > “Does this problem really exist?” and “Does our solution actually solve it?”

**Key Principle:**  
> Build to **learn**, not to **scale**.

---

## 💭 How Do Product Managers Think About MVPs?

- PMs use MVPs to **validate assumptions quickly** and **de-risk decisions**.  
- MVPs are **experiments**, not prototypes — they must test a **real hypothesis**.  
- Good MVPs:
  - Focus on **one clear learning goal**  
  - Deliver a **real user experience** (not just theoretical feedback)  
  - Measure a **defined success metric**  

- PM mindset:
  - **“What’s the fastest way to learn?”** instead of “What’s the fastest way to build?”
  - Use MVPs to test **behavior**, not opinions.

> Example: Instead of asking, “Would you use this feature?”, observe whether users actually click, sign up, or pay.

---

## 🧭 Seven Steps to Running an MVP Experiment

1. **Define Objective**  
   - What do you want to learn? (e.g., user need, feature adoption, pricing fit)

2. **Identify Assumptions**  
   - What must be true for your idea to succeed?

3. **Formulate Hypotheses**  
   - Convert assumptions into testable statements.  
   - Example: “If we offer free setup, 40% of users will complete onboarding.”

4. **Design the MVP**  
   - Choose the smallest experiment to test the hypothesis.  
   - Examples: landing page, email test, concierge MVP.

5. **Set Success Criteria (MCS)**  
   - Define the **Minimum Criterion for Success** before launching.  
   - Prevents bias in interpreting results.

6. **Run the Experiment**  
   - Launch quickly, collect behavioral data, and observe patterns.

7. **Analyze and Learn**  
   - Evaluate outcomes: Confirmed ✅ | Contradicted ❌ | Inconclusive ⚪  
   - Use insights to iterate or pivot.

---

## 🧠 Identify Your Assumptions

- Assumptions are **beliefs that must hold true** for your product to succeed.  
- Identifying them early helps you **prioritize risks** and design better experiments.

**Types of Assumptions:**
1. **Desirability:** Do users actually want this?  
2. **Feasibility:** Can we build it effectively?  
3. **Viability:** Is it sustainable or profitable?  
4. **Usability:** Can users understand and use it easily?

### Example Framework

| **Assumption Type** | **Example** | **Test Method** |
|----------------------|-------------|-----------------|
| Desirability | “Users want same-day delivery” |  landing page |
| Feasibility | “We can process requests in <2 hrs” | Internal prototype |
| Viability | “People will pay $10/month” | Pricing test |
| Usability | “Signup takes <2 min” | Usability test |

> Tip: Focus first on **the riskiest assumption** — the one most likely to break your business if false.

---

## 🧩 Case Study: Identify the Assumptions for Zirx

**Zirx** (example company) — an on-demand car service that picks up, parks, and returns your car.  
Identified assumptions include:

- **Customers feel real pain** finding parking in urban centers.  
- **Users trust** third-party drivers to handle their vehicles.  
- **Customers are willing to pay** for convenience.  
- **Drivers and logistics** can operate efficiently in dense cities.  
- **Demand density** is sufficient to make routes economical.  

> Key learning: List every assumption, no matter how obvious. You can’t test what you haven’t written down.

---

## ⚠️ Find the Riskiest Assumption of Them All

- The **riskiest assumption** is the one that, if false, makes the entire product fail.  
- Often related to **user demand**, **trust**, or **behavioral change**.

### How to Identify It
1. List all assumptions.
2. Rank them by **impact** (on product success) and **uncertainty** (how little you know).  
3. The assumption with **highest impact + highest uncertainty** = riskiest.

> Example (Zirx): “Will customers actually trust someone else to drive and park their car?”

---

## 🔲 Make Decisions: The Risk/Difficulty Square

Use this matrix to **prioritize what to test first**.

| | **Low Difficulty** | **High Difficulty** |
|--|--------------------|---------------------|
| **High Risk** | ✅ Test immediately | ⚠️ Requires creative experiment design |
| **Low Risk** | 🕒 Test later | 🚫 Probably not worth testing |

> Start with **High-Risk + Low-Difficulty** items.  
> They provide the fastest, cheapest learning loops.

---

## 🧪 What is a Hypothesis?

A **hypothesis** is a **testable statement** that connects an assumption to an expected outcome.  
It translates uncertainty into something measurable.

**Formula:**  
> _“If we [do X], then [Y] will happen because [reason].”_

Example:  
> “If we add a ‘Book Now’ button on the homepage, then at least 20% of visitors will click it, because they’re interested in on-demand parking.”

---

## 🧰 Put Together a Hypothesis

### Steps
1. **Start from your assumption.**  
   → “Customers want faster parking options.”
2. **Define an action or intervention.**  
   → “Show an ad describing our pickup & park service.”
3. **Define measurable outcome.**  
   → “At least 25% click ‘Learn More’.”

**Good Hypotheses Are:**
- Specific and measurable  
- Based on a single assumption  
- Disprovable (can fail)  
- Time-bound and clear

---

## 🚗 Follow Along: Identify Zirx’s Hypothesis

Example Hypothesis:
> “If we run Facebook ads targeting city commuters, 10% will sign up for early access because they want to avoid parking stress.”

- **Metric:** Click-through and sign-up rate  
- **Goal:** Validate **desirability** — whether the problem is worth solving  
- **Next Step:** Build a minimal landing page with signup form

---

## 🎯 What’s a Minimum Criterion for Success (MCS)?

- The **MCS** defines what success looks like for your MVP experiment.  
- It prevents **bias** in interpreting results after the fact.  
- MCS should be **quantitative**, **predefined**, and **tied to your hypothesis**.

**Example:**  
> “At least 20% of people who click the ad should sign up for early access.”

Without an MCS, any result can be rationalized as “interesting,” not “validated.”

---

## 📏 Create a Formula for Your MCS

To formalize success criteria:

> **MCS = Minimum % or Count that Confirms Hypothesis**

Example:
> “We will consider this test a success if at least 15% of visitors sign up, with a sample size of 200 visitors.”

### Tips
- Ensure **statistical significance** for meaningful conclusions  
- Document assumptions: sample size, timeframe, conversion goals  
- Always define MCS **before running** the experiment

---

## 💡 Make the Calculation for Startups

For early-stage startups, use **smaller, directional MCS** metrics:
- Focus on **signal, not scale**
- Measure **behavioral intent**, not growth metrics
- Example:
  - 20 out of 100 visitors sign up → positive signal  
  - 1 out of 100 → rethink messaging or value prop

> Early MCS ≠ proof of business model; it’s **evidence of user interest**.

---

## 📊 Evaluating Results and Learning from Them

1. **Compare results** against your hypothesis and MCS  
2. **Categorize outcomes:**
   - ✅ Confirmed  
   - ❌ Disproved  
   - ⚪ Inconclusive  
3. **Document findings:**
   - Key insights  
   - New assumptions  
   - Next experiments

> The goal is not to be right — it’s to **learn faster** and **reduce uncertainty**.

